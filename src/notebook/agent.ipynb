{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([True, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def is_different_from_default(result: Dict[str, str]) -> bool:\n",
    "    default_values = {\n",
    "        'date_range': 'N/A',\n",
    "        'from_date': 'N/A', \n",
    "        'to_date': 'N/A',\n",
    "        'product': 'All',\n",
    "        'product_detail': 'All',\n",
    "        'level': 'All',\n",
    "        'user': 'N/A'\n",
    "    }\n",
    "    \n",
    "    for key, default_value in default_values.items():\n",
    "        if key in result and result[key] != default_value:\n",
    "            return True\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {'date_range': 'N/A', 'from_date': 'N/A', 'to_date': 'N/A', 'product': 'Sportsbook', 'product_detail': 'All', 'level': 'All', 'user': 'N/A'}\n",
    "is_different_from_default(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\\\\\Desktop\\\\\\\\AlphaReportChatbot\\\\\\\\data\\\\\\\\customer.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m LEVEL \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSuper Agent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaster Agent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAgent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDirect Member\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# customer = pd.read_csv(r'/home/hoangtv/Desktop/Long/llm-tuning/data/customer.csv')\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m customer \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mAlphaReportChatbot\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mcustomer.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m USER \u001b[38;5;241m=\u001b[39m customer\u001b[38;5;241m.\u001b[39mdropna()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\leon.pham\\AppData\\Local\\anaconda3\\envs\\langchain\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leon.pham\\AppData\\Local\\anaconda3\\envs\\langchain\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\leon.pham\\AppData\\Local\\anaconda3\\envs\\langchain\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leon.pham\\AppData\\Local\\anaconda3\\envs\\langchain\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\leon.pham\\AppData\\Local\\anaconda3\\envs\\langchain\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\\\\\Desktop\\\\\\\\AlphaReportChatbot\\\\\\\\data\\\\\\\\customer.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PRODUCT = [\n",
    "    \"Sportsbook\", \"Number Game\", \"Virtual Sports\", \"Saba Casino\", \"RNG Keno\", \"AG Casino\",\n",
    "    \"Sportsbook 2\", \"RNG Slot\", \"Cricket\", \"Allbet\", \"Macau Games\", \"Cash Out\", \"Lottery\",\n",
    "    \"Voidbrige Jackpot\", \"Exchange\", \"PP\", \"Arcadia Gaming\", \"SG\", \"Saba Promotion\",\n",
    "    \"Virtual Games\", \"SA Gaming\", \"Table Game\", \"Live Casino\", \"SABA.games\", \"Togel 4D\",\n",
    "    \"RNG Games\", \"AE Sexy\", \"IBCBet Live Casino\", \"BBIN\", \"GPI\", \"WM\", \"ION\", \"RNG Casino\",\n",
    "    \"Saba Virtual Sports\", \"PG Soft\", \"Joker\", \"BG\", \"MaxGame\", \"Habanero\", \"CG\",\n",
    "    \"Sports Lottery\", \"PP Live Casino\", \"Vgaming\", \"AdvantPlay\", \"AdvantPlay Mini\", \n",
    "    \"Bitcoin\", \"FGG\", \"SEAL Entertainment\", \"MG-RNG\", \"Player Tips\", \"Jili\", \"YeeBet\", \n",
    "    \"UU SLOTS\", \"Live22\", \"WE Live Casino\", \"Saba Coins\", \"Yolo Play\", \"Nextspin\",\n",
    "    \"SABA xD\", \"FastSpin\", \"FA CHAI\", \"GPI Live Casino\", \"PLAYSTAR\", \"HOTDOG\", \"ON Casino\",\n",
    "    \"Smartsoft\", \"Playtech\", \"SABA Keno\", \"ASKMESLOT\", \"Funky Games\"\n",
    "]\n",
    "\n",
    "PRODUCT_DETAIL = [\n",
    "    \"SABA Basketball\", \"SABA Basketball PinGoal\", \"SABA E-Sports PinGoal\",\n",
    "    \"SABA Other Sports\", \"SABA Soccer\", \"SABA Soccer PinGoal\", \"SABA Tennis\",\n",
    "    \"Sportsbook\", \"Allbet Promotion\", \"PP Jackpot Contribution\", \"PP Jackpot Prize\",\n",
    "    \"SG Jackpot Contribution\", \"SG Jackpot Prize\", \"AE Sexy Lucky Draw\",\n",
    "    \"Vgaming Promotion Prize\", \"MG-RNG Promotion Prize\", \"Live22 Promotion Prize\",\n",
    "    \"FastSpin Promotion Prize\", \"FA CHAI Promotion Prize\", \"ASKMESLOT Promotion Prize\"\n",
    "]\n",
    "\n",
    "LEVEL = ['Super Agent', 'Master Agent', 'Agent', 'Direct Member']\n",
    "\n",
    "# customer = pd.read_csv(r'/home/hoangtv/Desktop/Long/llm-tuning/data/customer.csv')\n",
    "customer = pd.read_csv(r'D:\\\\Desktop\\\\AlphaReportChatbot\\\\data\\\\customer.csv')\n",
    "USER = customer.dropna()['username'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import requests\n",
    "import json\n",
    "from typing import Dict, Any, Optional\n",
    "import tqdm\n",
    "\n",
    "def llm(\n",
    "    user_prompt: str,\n",
    "    system_prompt: str,\n",
    "    format_schema: Dict[str, Any] = None,\n",
    "    api: str = 'https://ollama.selab.edu.vn',\n",
    "    endpoint: str = '/api/chat',\n",
    "    model: str = \"qwen2.5:14b\"\n",
    ") -> Dict[str, str]:\n",
    "    \n",
    "    headers = {\n",
    "      \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "\n",
    "    if endpoint == '/api/chat' and format_schema is not None:\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"format\": format_schema,\n",
    "            \"stream\": False\n",
    "        }\n",
    "    \n",
    "    elif endpoint == '/api/generate':\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": messages,\n",
    "            \"option\": {\n",
    "              \"temperature\": 0.4  \n",
    "            },\n",
    "            \"stream\": False\n",
    "        }\n",
    "\n",
    "    url = f'{api}{endpoint}'\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data, timeout=600)\n",
    "\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "\n",
    "    response_data = response.json()\n",
    "    \n",
    "    return json.loads(response_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Prepare function to call***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "def faiss_indexing(data_embeddings, path_save):\n",
    "    # indexing\n",
    "    cpu_index = faiss.IndexFlatIP(768)\n",
    "    for embedding in tqdm.tqdm(data_embeddings, colour='green', desc='Indexing'):\n",
    "        embedding = embedding.astype(np.float32).reshape(1, -1)\n",
    "        cpu_index.add(embedding)\n",
    "\n",
    "    # Save vector database\n",
    "    faiss.write_index(cpu_index, path_save)\n",
    "    \n",
    "     \n",
    "def save_pickle(\n",
    "    data, \n",
    "    file_path='bm25_index.pkl'\n",
    ") -> None:\n",
    "    # Save tokenized corpus into a pickle file\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def bm25_indexing(contents, path_save):\n",
    "    # indexing\n",
    "    tokenized_corpus = []\n",
    "    for doc in tqdm.tqdm(contents, colour='green'):\n",
    "        doc_tokens = doc.split()\n",
    "        tokenized_corpus.append(doc_tokens)\n",
    "\n",
    "    # Save into pickle file\n",
    "    save_pickle(tokenized_corpus, path_save)\n",
    "\n",
    "\n",
    "def embedding_data(\n",
    "    model_embedding,\n",
    "    contents\n",
    ") -> torch.Tensor:\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model_embedding.to(device)\n",
    "    embeddings = model.encode(contents)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_metadata = {\n",
    "    \"/get_winlost_report\": [\n",
    "        \"As an Agent, I want to retrieve Win/Loss reports via the chatbot so that I can track user performance and betting trends.\",\n",
    "        \"The chatbot should allow filtering by date, product, and user level. It should present results in a structured format (text/table).\",\n",
    "        \"The chatbot should support exporting reports (CSV/PDF).\",\n",
    "        \"The chatbot should allow agents to compare win/loss trends across different time periods to detect anomalies or high-risk users.\",\n",
    "        \"The report should highlight the biggest winners and losers in a given period.\",\n",
    "        \"The chatbot should notify agents of any unusual betting patterns that might require further investigation.\"\n",
    "    ],\n",
    "    \"/get_outstanding_report\": [\n",
    "        \"As an Agent, I want to access outstanding reports via the chatbot so that I can monitor unsettled transactions.\",\n",
    "        \"The chatbot should categorize outstanding reports based on transaction type (e.g., deposits, withdrawals, bets).\",\n",
    "        \"The chatbot should provide real-time updates on pending settlements.\",\n",
    "        \"The chatbot should allow filtering by user, date, and transaction amount to prioritize pending payments.\",\n",
    "        \"The chatbot should alert agents if outstanding transactions exceed a defined threshold.\"\n",
    "    ],\n",
    "    \"/get_statement_report\": [\n",
    "        \"As an Agent, I want to generate a statement report via the chatbot so that I can track transactions and financial summaries.\",\n",
    "        \"The chatbot should retrieve statements for a specific date range.\",\n",
    "        \"The chatbot should allow exporting statement reports in multiple formats (CSV, PDF).\",\n",
    "        \"The chatbot should summarize total deposits, withdrawals, and net balance for each statement period.\",\n",
    "        \"The chatbot should allow agents to generate reports for individual users or aggregated across all users.\",\n",
    "        \"The chatbot should provide a breakdown of different transaction categories (e.g., bet winnings, bonuses, refunds).\"\n",
    "    ],\n",
    "    \"/create_customer_information\": [\n",
    "        \"As an Agent, I want to create a new member via the chatbot so that I can quickly onboard new users.\",\n",
    "        \"The chatbot should collect required user information (name, contact, account type).\",\n",
    "        \"The chatbot should validate input data before submission to prevent duplicate or incorrect registrations.\",\n",
    "        \"The chatbot should confirm successful account creation and generate a welcome message.\",\n",
    "        \"The chatbot should assign default account limits based on user type.\",\n",
    "        \"The chatbot should support bulk user registration via file upload or batch processing.\"\n",
    "    ],\n",
    "    \"/get_member_details\": [\n",
    "        \"As an Agent, I want to retrieve member details via the chatbot so that I can verify user accounts.\",\n",
    "        \"The chatbot should allow searching by username or user ID.\",\n",
    "        \"The chatbot should display user level, account settings, and account status.\",\n",
    "        \"The chatbot should highlight any account restrictions or previous risk flags.\",\n",
    "        \"The chatbot should provide an account history summary, including past transactions and betting activities.\",\n",
    "        \"The chatbot should support multi-level filtering (e.g., by region, VIP status, risk category).\"\n",
    "    ],\n",
    "    \"/get_problem_accounts\": [\n",
    "        \"As an Agent, I want to view a list of problem accounts via the chatbot so that I can take corrective actions.\",\n",
    "        \"The chatbot should display problem accounts with their current status and reason for being flagged.\",\n",
    "        \"The chatbot should allow filtering by issue type (e.g., fraud suspicion, payment issues, excessive losses).\",\n",
    "        \"The chatbot should generate alerts for accounts that repeatedly trigger risk warnings.\",\n",
    "        \"The chatbot should provide recommendations on how to handle each problem account.\"\n",
    "    ],\n",
    "    \"/get_bet_tickets\": [\n",
    "        \"As an Agent, I want to search for specific bet tickets via the chatbot so that I can track individual bet details.\",\n",
    "        \"The chatbot should allow searching by ticket ID, user, and date.\",\n",
    "        \"The chatbot should display bet details, including bet type, odds, and payout status.\",\n",
    "        \"The chatbot should provide real-time updates on active bets.\",\n",
    "        \"The chatbot should allow agents to filter by bet outcome (e.g., won, lost, pending).\"\n",
    "    ],\n",
    "    \"/get_running_bets\": [\n",
    "        \"As an Agent, I want to retrieve running bets via the chatbot so that I can verify member betting behavior and forecast risk amount.\",\n",
    "        \"The chatbot should allow filtering by date, product, and user.\",\n",
    "        \"The chatbot should display bet details and status (e.g., pending, live, settled).\",\n",
    "        \"The chatbot should allow exporting bet reports for further analysis.\",\n",
    "        \"The chatbot should highlight bets with unusually high stakes.\"\n",
    "    ],\n",
    "    \"/get_cancelled_bets\": [\n",
    "        \"As an Agent, I want to view cancelled bets via the chatbot so that I can investigate betting issues.\",\n",
    "        \"The chatbot should display cancelled bets with reasons (e.g., voided, system error, user cancellation).\",\n",
    "        \"The chatbot should allow filtering by date and user.\",\n",
    "        \"The chatbot should identify patterns in cancelled bets (e.g., frequent cancellations by the same user).\",\n",
    "        \"The chatbot should generate reports on the impact of cancelled bets on total revenue.\"\n",
    "    ],\n",
    "    \"/get_bet_forecasting\": [\n",
    "        \"As an Agent, I want the chatbot to generate total bets so that I can mitigate potential losses.\",\n",
    "        \"The chatbot should summarize major bet types, the number of tickets, and stakes on running tickets of each event.\",\n",
    "        \"The chatbot should predict win/loss amounts based on current scores.\",\n",
    "        \"The chatbot should display the Total Forecast Win/Loss of each Live event.\",\n",
    "        \"The chatbot should alert agents when projected losses exceed a predefined threshold.\"\n",
    "    ],\n",
    "    \"/get_winlost_limit\": [\n",
    "        \"As an Admin, I want to get a member’s win/loss limit via the chatbot so that I can assess their risk level.\",\n",
    "        \"The chatbot should retrieve win/loss limit settings for specific members.\",\n",
    "        \"The chatbot should notify admins if a user approaches or exceeds their win/loss limit.\",\n",
    "        \"The chatbot should provide historical data on win/loss limit breaches.\",\n",
    "        \"The chatbot should allow admins to modify limits based on risk assessment.\"\n",
    "    ],\n",
    "    \"/set_risk_thresholds\": [\n",
    "        \"As an Agent, I want to set risk thresholds via the chatbot so that I can automatically flag risky bets.\",\n",
    "        \"The chatbot should allow setting limits for winning/losing thresholds.\",\n",
    "        \"The chatbot should trigger alerts when thresholds are exceeded.\",\n",
    "        \"The chatbot should log all risk threshold violations.\",\n",
    "        \"The chatbot should allow different threshold levels based on user category (e.g., high-risk users, VIP users).\"\n",
    "    ],\n",
    "    \"/analyze_betting_behavior\": [\n",
    "        \"As an Agent, I want to analyze user betting behavior via the chatbot so that I can identify potential problem gamblers or fraud cases.\",\n",
    "        \"The chatbot should generate a user profile based on betting patterns, including preferred sports, bet frequency, and risk levels.\",\n",
    "        \"The chatbot should detect unusual betting behaviors, such as excessive stakes or repeated last-minute bets.\",\n",
    "        \"The chatbot should segment users into different categories based on their betting style (e.g., casual, high-risk, professional).\",\n",
    "        \"The chatbot should provide trend analysis, showing how a user’s betting habits change over time.\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from the 🤗 Hub\n",
    "model_embedding = SentenceTransformer(\"hiieu/halong_embedding\")\n",
    "\n",
    "function_description = list(function_metadata.values())\n",
    "\n",
    "def flatten_list_2d(list_2d):\n",
    "    return [item for sublist in list_2d for item in sublist]\n",
    "\n",
    "function_description = flatten_list_2d(function_description)\n",
    "\n",
    "# Generate embeddings for the flattened descriptions\n",
    "embeddings = embedding_data(\n",
    "    model_embedding=model_embedding,\n",
    "    contents=function_description\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_description.index(function_metadata['/get_outstanding_report'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing: 100%|\u001b[32m██████████\u001b[0m| 69/69 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "faiss_indexing(embeddings, 'function_description.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32m██████████\u001b[0m| 69/69 [00:00<00:00, 199178.92it/s]\n"
     ]
    }
   ],
   "source": [
    "bm25_indexing(function_description, 'funtion_description.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Function calling***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(query):\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "    You are an AI assistant trained to extract keywords from user queries. Your task is to identify the most relevant and meaningful keywords from the given sentence. Focus on nouns, proper nouns, and key phrases that convey the core meaning of the query. Avoid extracting common stop words or irrelevant terms.\n",
    "\n",
    "    # **Key Guidelines:**\n",
    "    - Extract only the most relevant keywords.\n",
    "    - Focus on nouns, proper nouns, and key phrases.\n",
    "    - Avoid stop words or irrelevant terms.\n",
    "    - Ensure the extracted keywords are concise and meaningful.\n",
    "\n",
    "    # ***Example Scenarios:***\n",
    "    - ***Input***: \"Get me a Win Loss Detail Report for Direct Member who played Sportsbook.\"\n",
    "    - ***Output***: [\"Win Loss Detail Report\", \"Direct Member\", \"Sportsbook\"]\n",
    "\n",
    "    - ***Input***: \"Show me the betting trends for last month.\"\n",
    "    - ***Output***: [\"betting trends\", \"last month\"]\n",
    "\n",
    "    - ***Input***: \"Retrieve member details for user123.\"\n",
    "    - ***Output***: [\"member details\", \"user123\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    format_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"keywords\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"keywords\"]\n",
    "    }\n",
    "    \n",
    "    user_prompt = f'''\n",
    "        Please extract the keywords from this sentences: {query}\n",
    "    '''\n",
    "\n",
    "    keywords = llm(\n",
    "        user_prompt=user_prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        format_schema=format_schema\n",
    "    )\n",
    "\n",
    "    return keywords['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bin(path):\n",
    "    cpu_index = faiss.read_index(path) \n",
    "    return cpu_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_pickle(file_path: str):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "\n",
    "def bm25_keyword_search(query, corpus, topk):\n",
    "\n",
    "  bm25 = BM25Okapi(corpus)\n",
    "  tokenized_query = extract_keywords(query).keywords\n",
    "  function_names = bm25.get_top_n(tokenized_query, corpus, n=topk)\n",
    "\n",
    "  return function_names\n",
    "\n",
    "def semantic_keyword_search(query, corpus, model_embedding, cpu_index, topk):\n",
    "  keywords = extract_keywords(query).keywords\n",
    "\n",
    "  function_names = []\n",
    "  for keyword in keywords:\n",
    "    prompt_embedding = model_embedding.encode([keyword])\n",
    "    prompt_embedding = np.array(prompt_embedding)\n",
    "    scores, indices = cpu_index.search(prompt_embedding, topk)\n",
    "    contexts = [', '.join(data[i]['headers']) + ' ' + data[i]['content'] for i in indices.flatten().tolist()]\n",
    "    function_names.extend(contexts)\n",
    "\n",
    "  return function_names\n",
    "\n",
    "\n",
    "def semantic_search(query, corpus, model_embedding, cpu_index, topk):\n",
    "  prompt_embedding = model_embedding.encode([query])\n",
    "  prompt_embedding = np.array(prompt_embedding)\n",
    "  scores, indices = cpu_index.search(prompt_embedding, topk)\n",
    "  function_names = [', '.join(data[i]['headers']) + ' ' + data[i]['content'] for i in indices.flatten().tolist()]\n",
    "\n",
    "  return function_names\n",
    "\n",
    "def hybrid_search(query, topk):\n",
    "  \n",
    "  bm25_index = load_pickle('function_description.pkl')\n",
    "  faiss_index = load_bin('function_description.index')\n",
    "  \n",
    "  \n",
    "  \n",
    "  bm25_contexts = bm25_keyword_search(query, bm25_index, topk)\n",
    "  halong_contexts = semantic_keyword_search(query, model_embedding, faiss_index, topk)\n",
    "  semantic_contexts = semantic_search(query, model_embedding, faiss_index, topk)\n",
    "\n",
    "  function_names = bm25_contexts + halong_contexts + semantic_contexts\n",
    "  function_names = list(set(function_names))\n",
    "\n",
    "  return function_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "import torch.nn as nn\n",
    "\n",
    "model_reranking = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2', max_length=512)\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "def hybrid_search_with_reranking(query, topk):\n",
    "  contexts = hybrid_search(query, topk)\n",
    "  formated_contexts = [[query, context] for context in contexts]\n",
    "\n",
    "  scores = model_reranking.predict(formated_contexts, activation_fct=sigmoid)\n",
    "  top_k_values = np.sort(scores)[-topk:][::-1]\n",
    "\n",
    "  top_k_indices = np.argsort(scores)[-topk:][::-1]\n",
    "  print(top_k_values)\n",
    "\n",
    "  contexts = np.array(contexts)\n",
    "  best_contexts = contexts[top_k_indices].tolist()\n",
    "\n",
    "  return best_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_calling(\n",
    "    user_prompt: str,\n",
    "    system_prompt: str,\n",
    "    vector_db: faiss.IndexFlatL2,\n",
    "    db_contains_function_name: Dict[str] \n",
    ") -> str:\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Build conversation chatbot***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_agent(\n",
    "    query: str,\n",
    "    parameters: Optional[Dict[str, str]]\n",
    ") -> Dict[str, str]:\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "    You are a highly intelligent, helpful, and friendly assistant, designed to engage in natural and meaningful conversations, especially with users from the Alpha department. Your responses should be conversational, engaging, and context-aware. Always prioritize clarity and helpfulness.\n",
    "    When a user provides parameters for a function call, always confirm their intent before proceeding.\n",
    "    Adapt your tone to match the user's style, making interactions feel seamless and natural.\n",
    "    Strive for accuracy and relevance in your responses, ensuring they are both insightful and easy to understand.\n",
    "    If necessary, ask thoughtful follow-up questions to clarify user needs and improve the conversation.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    You are interacting with a user from the Alpha department. Your primary goal is to provide intelligent, engaging, and context-aware responses while maintaining a conversational and friendly tone.\n",
    "\n",
    "    # **Key Guidelines:**\n",
    "    - If the user provides parameters for a function call, **confirm their intent before proceeding**.\n",
    "    - Match the user’s tone and communication style for a natural conversation.\n",
    "    - Ensure responses are **clear, helpful, and contextually relevant**.\n",
    "    - When necessary, ask **follow-up questions** to clarify user needs.\n",
    "    \n",
    "    # ***User's query***\n",
    "    {query}\n",
    "    \n",
    "    # ***Parameters detected from the user's query***\n",
    "    {parameters}\n",
    "    \n",
    "    # ***Example Scenarios:***\n",
    "    - ***User***: \"Get me a Win Loss Detail Report on day 10\"\n",
    "    - ***Assistant***: \"Got it. You would like to generate a Win/Loss Detail Report for the most recent day (day 10 of March 2025).\\n\\nTo proceed, I need to clarify:\\n- The date range should be from March 10, 2025, to March 10, 2025.\\n- Do you want to apply any filters such as user level or product type? If not, we will generate the report without filters.\\n\\nPlease confirm these details before I proceed with generating the report.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm(\n",
    "        user_prompt=user_prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        format_schema=None,\n",
    "        endpoint='/api/generate'\n",
    "    )\n",
    "    \n",
    "    return response['response']\n",
    "\n",
    "\n",
    "def confirmation_agent(\n",
    "    query: str\n",
    ") -> Dict[str, str]:\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "    You are a confirmation agent designed to validate user intent and confirm their requests before proceeding. Your primary goal is to ensure clarity and accuracy in understanding the user's needs.\n",
    "\n",
    "    # **Key Guidelines:**\n",
    "    - Always confirm the user's intent explicitly and clearly.\n",
    "    - Provide a concise summary of the user's request for confirmation.\n",
    "    - If the user's intent is unclear, ask clarifying questions to ensure mutual understanding.\n",
    "    - Respond in a polite and professional tone, maintaining a focus on accuracy and helpfulness.\n",
    "\n",
    "    # ***Example Scenarios:***\n",
    "    - ***User***: \"I want to delete all my data.\"\n",
    "    - ***Assistant***: \"To confirm, you would like to permanently delete all your data. This action cannot be undone. Please confirm if you would like to proceed.\"\n",
    "    - ***User***: \"Yes, delete it.\"\n",
    "    - ***Assistant***: \"Understood. I will proceed with deleting all your data. Please wait a moment while I complete this action.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    You are interacting with a user who has made a request. Your primary goal is to confirm their intent explicitly and clearly. Provide a concise summary of the user's request and ask for confirmation.\n",
    "\n",
    "    # **Key Guidelines:**\n",
    "    - Confirm the user's intent explicitly and clearly.\n",
    "    - Provide a concise summary of the user's request for confirmation.\n",
    "    - If the user's intent is unclear, ask clarifying questions to ensure mutual understanding.\n",
    "    - Respond in a polite and professional tone, maintaining a focus on accuracy and helpfulness.\n",
    "\n",
    "    # ***User's query***\n",
    "    {query}\n",
    "\n",
    "    # ***Example Scenarios:***\n",
    "    - ***User***: \"I want to delete all my data.\"\n",
    "    - ***Assistant***: \"To confirm, you would like to permanently delete all your data. This action cannot be undone. Please confirm if you would like to proceed.\"\n",
    "    - ***User***: \"Yes, delete it.\"\n",
    "    - ***Output***: {\"is_confirmed\": 1}\n",
    "\n",
    "    - ***User***: \"I want to delete all my data.\"\n",
    "    - ***Assistant***: \"To confirm, you would like to permanently delete all your data. This action cannot be undone. Please confirm if you would like to proceed.\"\n",
    "    - ***User***: \"No, I changed my mind.\"\n",
    "    - ***Output***: {\"is_confirmed\": 0}\n",
    "\n",
    "    - ***User***: \"Generate a sales report for last month.\"\n",
    "    - ***Assistant***: \"To confirm, you would like to generate a sales report for the last month. Please confirm if this is correct.\"\n",
    "    - ***User***: \"Yes, that's correct.\"\n",
    "    - ***Output***: {\"is_confirmed\": 1}\n",
    "\n",
    "    - ***User***: \"Generate a sales report for last month.\"\n",
    "    - ***Assistant***: \"To confirm, you would like to generate a sales report for the last month. Please confirm if this is correct.\"\n",
    "    - ***User***: \"No, I meant for the last week.\"\n",
    "    - ***Output***: {\"is_confirmed\": 0}\n",
    "    \"\"\"\n",
    "    \n",
    "    format_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"is_confirmed\": {\n",
    "                \"type\": \"interger\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = llm(\n",
    "        user_prompt=user_prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        format_schema=format_schema\n",
    "    )\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "\n",
    "def entity_extraction_agent(\n",
    "    query: str,\n",
    "    format_schema: Dict[str, Any]\n",
    ") -> Dict[str, str]:\n",
    "    \n",
    "    system_prompt = \"You are an AI assistant majoring for Named Entity Recognition trained to extract entity and categorize queries for Winlost Report Detail\"\n",
    "  \n",
    "    user_prompt = f\"\"\"\n",
    "    # Information: \n",
    "        - Extract the most relevant keywords from the following sentence: '{query}'. \n",
    "        - Focus on important nouns that convey the core meaning. \n",
    "        - Detect any words related to dates such as tomorrow, today, last week, next year, so on, following the example below.\n",
    "        - If no relevant keywords are detected, return 'All' (except for dates, you must fill 'N/A').\n",
    "        - Here is the list of product and product detail you should detect:\n",
    "            ### PRODUCT = {PRODUCT}\n",
    "            ### PRODUCT_DETAIL = {PRODUCT_DETAIL}\n",
    "            ### LEVEL = {LEVEL}\n",
    "    \n",
    "    # Note:\n",
    "        - Occasionaly, \"user\" keyword may appear some words such as \"master*\", \"super*\", \"admin*\", \"user*\", \"agent*\" where * is any characters.\n",
    "        - In addition, when you detect the product and product_detail name, the result must not contain any words such as Product, Product Detail in the answer for parameters\n",
    "    \n",
    "    # Example:\n",
    "        ***Example 1***:\n",
    "        ## User: Get me a Win Loss Detail Report on day 10\n",
    "        ## Output:\n",
    "        {{\n",
    "            \"date_range\": \"day 10\",\n",
    "            \"product\": \"All\",\n",
    "            \"product_detail\": \"All\",\n",
    "            \"level\": \"All\",\n",
    "            \"user\": \"N/A\"\n",
    "        }}\n",
    "        \n",
    "        ***Example 2***:\n",
    "        ## User: Get me a Win Loss Detail Report for Direct Member who played Product Detail Sportsbook in Sportsbook Product from 01/02/2024 to 15/02/2024\n",
    "        ## Output:\n",
    "        {{\n",
    "            \"date_range\": \"01/02/2024 to 15/02/2024\",\n",
    "            \"product\": \"Sportsbook\",\n",
    "            \"product_detail\": \"Sportsbook\",\n",
    "            \"level\": \"Direct Member\",\n",
    "            \"user\": \"N/A\"\n",
    "        }}\n",
    "        \n",
    "        ***Example 3***:\n",
    "        ## User: Get me a Win Loss Detail Report for Super Agent who played Product Detail SABA Basketball in SABA Basketball Product from 01/02/2024 to 15/02/2024\n",
    "        ## Output:\n",
    "        {{\n",
    "            \"date_range\": \"01/02/2024 to 15/02/2024\",\n",
    "            \"product\": \"SABA Basketball\",\n",
    "            \"product_detail\": \"SABA Basketball\",\n",
    "            \"level\": \"Super Agent\",\n",
    "            \"user\": \"N/A\"\n",
    "        }}\n",
    "        \n",
    "        ***Example 4***:\n",
    "        ## User: Win/Loss details for Product Sportsbook\n",
    "        ## Output:\n",
    "        {{\n",
    "            \"date_range\": \"All\",\n",
    "            \"product\": \"Sportsbook\",\n",
    "            \"product_detail\": \"All\",\n",
    "            \"level\": \"All\",\n",
    "            \"user\": \"N/A\"\n",
    "        }}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm(\n",
    "        user_prompt=user_prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        format_schema=format_schema\n",
    "    )\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "\n",
    "def winlost_report_extraction(\n",
    "    query: str\n",
    ") -> Dict[str, str]:\n",
    "    \n",
    "    format_schema = {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"date_range\": {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"product\": {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"product_detail\": {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"level\": {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"user\": {\n",
    "          \"type\": \"string\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\n",
    "        \"date_range\",\n",
    "        \"product\",\n",
    "        \"product_detail\",\n",
    "        \"level\",\n",
    "        \"user\"\n",
    "      ]\n",
    "    }\n",
    "    \n",
    "    entites = entity_extraction_agent(\n",
    "        query=query,\n",
    "        format_schema=format_schema\n",
    "    )\n",
    "    \n",
    "    return entites\n",
    " \n",
    "    \n",
    "def entity_verification_agent(\n",
    "    **entity\n",
    ") -> Dict[str, str]:\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def entity_verification_algorithm(\n",
    "    **entity\n",
    ") -> Dict[str, str]:\n",
    "    pass\n",
    "\n",
    "\n",
    "def detect_is_new_session_agent(\n",
    "    query: str\n",
    ") -> Dict[str, str]:\n",
    "    pass\n",
    "\n",
    "def datetime_convert_agent(\n",
    "    query: str\n",
    ") -> Dict[str, str]:\n",
    "    pass\n",
    "\n",
    "def detect_and_translate(\n",
    "    query: str\n",
    ") -> str:\n",
    "    \n",
    "    pass\n",
    "\n",
    "def update_historical_conversation(\n",
    "    **conversation: Any\n",
    ") -> None:\n",
    "    \n",
    "    pass\n",
    "\n",
    "def clear_historical_conversation() -> None:\n",
    "    \n",
    "    pass\n",
    "\n",
    "def alpha_report_chatbot(\n",
    "    query: str\n",
    ") -> Dict[str, str]:\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Let's make sure everything is in order before we proceed with your Win/Loss Report.\n",
      "\n",
      "Here’s a quick summary of what you’ve requested:\n",
      "\n",
      "- **Date Range:** 10/04/2025 - 10/04/2025\n",
      "- **Product:** Virtual Sports\n",
      "- **Product Detail:** SABA Basketball\n",
      "- **Level:** Super Agent\n",
      "- **User:** None\n",
      "\n",
      "Is this correct? If everything looks good, I’ll generate the report for you.\n",
      "\n",
      "---\n",
      "\n",
      "If there’s anything else we need to adjust or add, let me know! 😊\n"
     ]
    }
   ],
   "source": [
    "print(\"Sure! Let's make sure everything is in order before we proceed with your Win/Loss Report.\\n\\nHere’s a quick summary of what you’ve requested:\\n\\n- **Date Range:** 10/04/2025 - 10/04/2025\\n- **Product:** Virtual Sports\\n- **Product Detail:** SABA Basketball\\n- **Level:** Super Agent\\n- **User:** None\\n\\nIs this correct? If everything looks good, I’ll generate the report for you.\\n\\n---\\n\\nIf there’s anything else we need to adjust or add, let me know! 😊\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
